# Confusion Maxtrix

A confusion matrix is a tble t hat is often used to describe the performance of a classification model on a set of test data for which the true values are known.

<img width="773" alt="image" src="https://github.com/tan200224/Blog/assets/68765056/03bb73e0-9ec0-4da7-9bf1-344fccf6f0f5">


### Binary Classification Problems

<img width="742" alt="image" src="https://github.com/tan200224/Blog/assets/68765056/67ef40e4-f155-4a6d-ae79-f1b437578586">

Recall or  true positive rate or sensitivity: True positives/True Positive Labels = 90/100 = 0.9. When it is a Yes, how often do we predict Yes.
False Positive Rate = False positives/True negative Labels = 5/45 = 0.11. When it is actually no, how often does it predict Yes. 
True Negative Rate = True Nagtive/True Negative labels. When it is actually no, how often does it predict No. 
Precision - True Positive / Predicted Yes = 90/95. When it is Yes, how often it is right?
F1 Score - is the harmonic mean of both precision and recall. It is more informative than accuracy alone as it. = (precision x Recall) / (precision + recall)




